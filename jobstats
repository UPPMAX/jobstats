#!/usr/bin/env perl

# Discover Uppmax jobstats files, make preliminary analysis of resource usage,
# and invoke plot_jobstats for further analysis and plot production.
#
# Latest is available at https://github.com/douglasgscofield/uppmax
#
# DONE: (from lka) help information about flag:number:number values
# DONE: (from lka) verbose flag to be wordy about flag output?
# DONE: (from lka) get current cluster from $SNIC_RESOURCE
# DONE: (from lka) memory flag only included if already a cores issue
# DONE: (from lka) problem with plots from 100-node tintin job, and with 10-node tintin job
# DONE: include more about user, jobstate in plot and table
# DONE: add -A to call finishedjobinfo with project name
# DONE: add -b to produce bigger plot with twice the typical dimensions
# DONE: add text about using mode 2 to produce plot for currently-running job
# DONE: add option to produce plot for running job (using squeue for discovery)
# DONE: remove documentation of --source option
# DONE: wrap jobnames in single quotes for output and passing to plot_jobstats
# DONE: for multiple jobs, bundle into a single finishedjobinfo -j
# DONE: static max RAM usage from finishedjobinfo, so make its default be to not use '-q'
# DONE: switch to %fields hash instead of @fields array
# TODO: complete $o_plot_stdin to enable single call to plot_jobstats with multiple job info fed on stdin
# TODO: YES implement Martin's efficiency statistic and include in output
# TODO: YES together with above, provide deeper quantitative information about efficiency on request
# TODO: YES coordinate with slurm.epilog for a -C flag
#
# TODO: ? add usage of Martin's sqlite3 db if it is available (i.e. if we are on milou-b)
# TODO: ? is there another, deeper, location for jobstats files we can check?
#

my $version = "2016-10-23";

my $enable_multijobs = 1;
my $enable_plot_stdin = 0;

use strict;
use warnings;
# use diagnostics;

use Getopt::Long qw(:config no_ignore_case);
use File::Basename;
use Cwd 'abs_path';

my $CWD = dirname(abs_path($0));
my $NAME = basename($0);

# valid values for --source
# "fji" for finishedjobinfo, "db" for Martin's database
my %source_values = ( fji => "fji", finishedjobinfo => "fji", squeue => "squeue", db => "db" );
# flag passed to $o_plot_jobstats for each source type
my %source_R_flags = ( fji => "--fji", squeue => "--squeue", db => "--db" );

# options and user values
my $o_cluster = $ENV{'SNIC_RESOURCE'} ? $ENV{'SNIC_RESOURCE'} : 'milou';  # -M
my $o_running         = 0;  # -r | --running, also sets $o_source to "squeue"
my $o_source          = "fji";
my $o_plot            = 0;  # -p
my $o_bigplot         = 0;  # -b
my $o_memory          = 0;  # -m
my $o_cpufree         = 3.0;  # --cpu-free
my $o_project;              # -A
my $o_nodes;                # -n
my $o_stdin           = 0;  # -- for finishedjobinfo info on stdin
my $o_plot_stdin      = 0;  # to send job-specific details to plot_jobstats on its stdin
my $o_header          = 0;  # -d
my $o_verbose         = 0;  # -v
my $o_quiet           = 0;  # -q | --quiet
my $o_Quick           = 0;  # -Q | --Quick to use finishedjobinfo -q
my $o_finishedjobinfo = "/sw/uppmax/bin/finishedjobinfo"; # -f
my $o_no_multijobs    = 0;  # --no-multijobs
my $o_plot_jobstats   = "$CWD/plot_jobstats"; # -P  # default plot_jobstats is in our directory
my $o_prefix          = "/sw/share/slurm"; # -x
my $o_hardprefix      = ""; # -X
my $o_help;                 # -h
my $o_debug           = 0;  # --debug
my @nodes;    # after unpacking finishedjobinfo or -n
my @jobs;     # user-specified jobids

my $usage = "
$NAME  -p [options] [ -M cluster ] [ jobid [ jobid ... ] | -A project | - ]

Discover jobstats for the specified job(s) on the specified cluster.  Cluster
defaults to the value of \$SNIC_RESOURCE ('$ENV{'SNIC_RESOURCE'}' on the current system) if
not specified.

With the -p/--plot option, a plot is produced from the jobstats for each
jobid.  Plots contain one panel per booked node showing CPU (blue) and memory
usage (black) traces and include text lines indicating the job number, cluster,
end time and duration, user, project, job name, and usage flags (more on those
below).  For memory usage, one or two traces are shown: a solid black line
shows instantaneous memory usage, and a dotted black line shows overall maximum
memory usage if this information is available.

Plots are saved to the current directory with the name

    cluster-project-user-jobid.png

Note that not all jobs will produce jobstats files, particularly if the job was
cancelled or ran for less than 5 minutes.  Also, if a job booked nodes
inefficiently by not using nodes it asked for, jobstats files will not be
available for the booked but unused nodes.

JOBSTATS DISCOVERY
------------------

There are five modes for discovery, depending on what the user provides on the
command line: (1) discovery by job number for a completed job; (2) discovery by
job number for a currently running job; (3) discovery by node and job number,
for a completed or running job; (4) discovery by project; or (5) discovery via
information provided on 'stdin'.  In each of the example command lines below, the
-p/--plot option requests that plots of job resource usage are created.

Mode 1:  $NAME -p jobid1 jobid2 jobid3
-------
Job numbers valid on the cluster.  finishedjobinfo is used to determine further
information for each job.  If multiple queries are expected, it might be quicker
to run finishedjobinfo yourself separately, see Mode 5 below.  See Mode 2 for a
currently running job.

Mode 2:  $NAME -p -r jobid1 jobid2 jobid3
-------
Job numbers of jobs currently running on the cluster.  The SLURM squeue tool is
used to determine further information for each running job.

Mode 3:  $NAME -p -n m15,m16 jobid
-------
finishedjobinfo is *not* called and Uppmax's stored job statistics files are
discovered directly.  If you know which node(s) your job ran on or which nodes
you are interested in, this will be much faster than Mode 1.

Mode 4:  $NAME -p -A project
-------
When providing a project name that is valid for the cluster, finishedjobinfo is
used to determine further information on jobs run within the project.  As for
Mode 1, this can be rather slow.  Furthermore only finishedjobinfo defaults for
time span etc. are used for job discovery.  If multiple queries are expected or
additional finishedjobinfo options are desired, see Mode 5 below.

Mode 5:  finishedjobinfo project | $NAME - -p
-------
Accept input on stdin formatted like finishedjobinfo output.  The long form of
this option is '--stdin'.  This mode can be especially useful if multiple
queries of the same job information are expected.  In this case, save the
output of a single comprehensive finishedjobinfo query, and extract the parts
of interest and present them to this script on stdin.  For example, to produce
analyses of all completed jobs in a project during the current calendar year,
and produce separate tarballs analysing all jobs and providing jobstats plots
for each user during this same period:

     finishedjobinfo -y project > proj-year.txt
     grep 'jobstat=COMPLETED' proj-year.txt | $NAME - > all-completed-jobs.txt
     grep 'username=user1' proj-year.txt | $NAME - -p > user1-jobs.txt
     tar czf user1-jobs.tar.gz user1-jobs.txt *-project-user1-*.png
     grep 'username=user2' proj-year.txt | $NAME - -p > user2-jobs.txt
     tar czf user2-jobs.tar.gz user2-jobs.txt *-project-user2-*.png
     ...

COMMAND-LINE OPTIONS
--------------------

    -p | --plot        Produce CPU and memory usage plot for each jobid

    -r | --running     Jobids are for jobs currently running on the cluster. The
                       SLURM squeue tool is used to discover further information
                       for the running jobs, and the rightmost extent of the plot
                       produced will reflect the scheduled end time of the job.

    -A project         Project valid on the cluster.  finishedjobinfo is used to
                       discover jobs for the project.  See further comments 
                       under 'Mode 4' above.

    -M cluster         Cluster on which jobs were run [default current cluster]

    -n node[,node...]  Cluster node(s) on which the job was run.  If specified,
                       then the finishedjobinfo script is not run and discovery
                       is restricted to only the specified nodes.  Nodes can be 
                       specified as a comma-separated list of complete node 
                       names, or using the finishedjobinfo syntax:
                             m78,m90,m91,m92,m100  or  m[78,90-92,100]
                       Nonsensical results will occur if the syntaxes are mixed.

    - | --stdin        Accept input on stdin formatted like finishedjobinfo 
                       output.  The short form of this option is a single dash 
                       '-'.
                       
    -m | --memory      Always include memory usage flags in output.  Default 
                       behaviour is to include memory usage flags only if CPU 
                       usage flags are also present.

    -v | --verbose     Be wordy when describing flag values.

    -b | --big-plot    Produce 'big plot' with double the usual dimensions.
                       This implies '-p/--plot'.

    -q | --quiet       Do not produce table output

    -Q | --Quick       Run finishedjobinfo with the -q option, which is slightly
                       faster but does not include SLURM's record of maximum
                       memory used. With this option, memory usage analyses can
                       only rely upon what is reported at 5-minute intervals,
                       and the trace of maximum memory used (dotted black line)
                       is not produced.

    -d                 Produce a header for table output

    -h | --help | -?   Produce detailed help information

";

my $detailed_usage = "ADDITIONAL OPTIONS
------------------

The following command-line options are generally only useful for Uppmax staff.

    --cpu-free FLOAT   Maximum CPU busy percentage for the CPU to count as
                       free at that sampling time.  Default is $o_cpufree %.
    -x directory       Directory prefix to use for jobstats files.  Default is
                       '$o_prefix', and directory structure is 

                       <prefix>/<cluster>/uppmax_jobstats/<node>/<jobid>

    -X directory       Hard directory prefix to use for jobstats files.  
                       Jobstats files are assumed available directly: 
                           '<hard-prefix>/<jobid>'
    --no-multijobs     Run finishedjobinfo separately for each jobid, rather
                       than once with all jobids bundled into one -j option
    -f file            finishedjobinfo script [default is '$o_finishedjobinfo']
    -P file            plot_jobstats script [default is '$o_plot_jobstats']

FURTHER DETAILS
---------------

This script produces two types of output.  If the -p/--plot command line option
is provided, a plot is created of core and memory usage across the life of the
job.  The name of the file produced has the format:

    cluster-project-user-jobid.png

Unless the -q/--quiet option is provided, a table is also produces containing
lines with the following tab-separated fields:

  jobid cluster jobstate user project endtime runtime flags booked cores node[,node...] jobstats[,jobstats...] 

Field contents:

  jobid    : Job ID
  cluster  : Cluster on which the job was run
  jobstate : End status of the job: COMPLETED, RUNNING, FAILED, TIMEOUT, CANCELLED
  user     : Username that submitted the job
  project  : Project account under which the job was run
  endtime  : End time of the job (with -n/--node, this is '.')
  runtime  : Runtime of the job (with -n/--node, this is '.')
  flags    : Flags indicating various types of resource underutilizations
  booked   : Number of booked cores (with -n/--node, this is '.')
  maxmem   : Maximum meory used as reported by SLURM (if unavailable, this is '.')
  cores    : Number of cores represented in the discovered jobstats files.
  node     : Node(s) booked for the job, expanded into individual node names,
             separated by commas; if no nodes were found, this is '.'.  
             The nodes for which jobstats files are available are listed first.
  jobstats : jobstats files for the nodes, in the same order the nodes are 
             listed, separated by commas; if no jobstats files were discovered,
             this is '.'

If -r/--running was used, an additional field is present:

  timelimit_minutes : The time limit of the job in minutes

FLAGS
-----

An important part of $NAME output are usage flags.  These provide indications
that booked resources -- processor cores or memory -- might have been
underused.

In both plot and table output, flags are a comma-separated list of cautions
regarding core and/or memory underutilisation.  The appearance of a flag does
not necessarily mean that resources were used incorrectly.  It depends upon the
tools being used and the contents of the SLURM header, and also depends upon
the job profile.  Because usage information is gathered every 5 minutes, higher
transient usage of cores or memory may not be captured in the log files.

Flags most likely to represent real overbooking of resources are
nodes_overbooked, overbooked, !!half_overbooked, !!severely_overbooked, and
!!swap_used.

For multinode jobs, flags other than nodes_overbooked are determined based only
on the usage of the first node.  Multinode jobs require careful analysis so as
to not waste resources unnecessarily, and it is a common mistake among
beginning Uppmax users to book multiple nodes and run tools that cannot use
more than the first.  In this case, nodes_overbooked will appear.

Some flags have a threshold below which they appear.  The default format is
generally 'flag:value-booked:value-used'.

  nodes_overbooked : nodes booked : nodes used
      More nodes were booked than used
  overbooked : % used (if < 80%)
      The maximum percentage of booked cores and/or memory that was used
  !!half_overbooked
      No more than 1/2 of both cores and memory of a node was used; consider booking 
      half a node instead.
  !!severely_overbooked
      No more than 1/4 of both cores and memory of a node was used, examine your job
      requirements closely.
  !!swap_used
      Swap storage was used at any point within the job run
  node_type_overbooked : type booked : type used
      A fat node was requested that was larger than was needed.  This flag may be
      produced spuriously if SLURM ran the job on a fat node when a fat node was not
      requested by the user.
  cores_overbooked : cores booked : cores used
      More cores were booked than used (if < 80%)
  mem_overbooked : GB booked : GB used
      More memory was available than was used (if < 25% and more than one core).
  core_mem_overbooked : GB in used cores : GB used
      Less memory was used than was available in the cores that were used (if < 50%).

By default no flags are indicated for jobs with memory-only cautions except for
swap usage, because it is common for jobs to heavily use processor cores
without using a sizable fraction of memory.  Use the -m/--memory option to
include flags for memory underutilisation when those would be the only flags
produced.

More verbose flags are output with the -v/--verbose option.


Script:   $0
Version:  $version

";

GetOptions("M=s"          => \$o_cluster, 
           "r|running"    => \$o_running,
           "n=s"          => \$o_nodes,
           "A=s"          => \$o_project,
           ""             => \$o_stdin,
           "stdin"        => \$o_stdin,
           #"source"       => \$o_source,
           "memory"       => \$o_memory,
           "verbose"      => \$o_verbose,
           "plot"         => \$o_plot,
           "big-plot"     => \$o_bigplot,
           "d"            => \$o_header,
           "cpu-free=f"   => \$o_cpufree,
           "quiet"        => \$o_quiet,
           "Quick"        => \$o_Quick,
           "x=s"          => \$o_prefix,
           "X=s"          => \$o_hardprefix,
           "f=s"          => \$o_finishedjobinfo,
           "P=s"          => \$o_plot_jobstats,
           "debug"        => \$o_debug,
           "no-multijobs" => \$o_no_multijobs,
           "help|?"       => \$o_help) or die "$usage";
die "$usage$detailed_usage" if $o_help;

die "stdin to plot_jobstats not yet implemented" if $o_plot_stdin or $enable_plot_stdin;
$o_plot_stdin = 0 if $o_no_multijobs;

$o_plot ||= $o_bigplot;  # set $o_plot if $o_bigplot set

@jobs = @ARGV;

($o_cluster and ($o_stdin or $o_project or scalar(@jobs) >= 1)) or die "\n***\n*** At least one jobid or -A or --stdin is required.\n***\n$usage";

my $has_multijobs = ($enable_multijobs and ! $o_no_multijobs and ! $o_running and scalar(@jobs) > 1) ? 1 : 0;

#$o_source = defined($source_values{$o_source}) ? $source_values{$o_source} : die "unrecognised --source value '$o_source'";

my $PREFIX = ( $o_hardprefix ? $o_hardprefix : "$o_prefix/$o_cluster/uppmax_jobstats" );

$o_stdin or ($o_running or (-f "$o_finishedjobinfo" or die "\n***\n*** no valid method for discovery (is the finishedjobinfo script '$o_finishedjobinfo' not available?)\n***"));

-d "$PREFIX" or die "\n***\n*** Jobstats directory prefix '$PREFIX/' is not a directory.\n***\n$usage";

my $username = scalar getpwuid $<;
my @user_groups = map { scalar getgrgid $_ } split ' ', $(;
my $in_staff = (scalar(grep /^staff$/, @user_groups) or ($username eq "root"));

$o_source = "squeue" if $o_running;

my $jobcount = 0;
my $jobcount_notrun = 0;
my $no_jobstats_file = 0;

# field order in output table
my @field_order = qw/ jobid cluster jobstate user project jobname endtime runtime flag_list booked maxmem core_list node_list file_list timelimit_minutes /;

sub parseSqueueLine($);
sub runningJobInfo($);
sub parseFinishedjobinfoLine($);
sub fieldNamesInOrder($);
sub fieldContentsInOrder($);
sub jobInfo($);
sub parseNodes($);
sub getJobstatsFiles($$@);
sub getJobstatsFileCoreCount($);
sub reorderNodeList($$);
sub runPlotJobstats($);
sub print_JobNotRun($);
sub print_JobRun($);

# parse a single runningJobInfo-formatted line and return a hash
sub parseSqueueLine($) {
    my $keyvals = shift;
    chomp $keyvals;
    return(0) if ! $keyvals;
    my %h = $keyvals =~ /([^ ]+)=([^ ]+)/g;
    $h{end_time} = "'$h{end_time} (sched)'";
    $h{jobname} = "'$h{jobname}'";
    # calculate timelimit_minutes
    my @tl = split(/[-:]/, $h{timelimit});  # days-hh:mm:ss
    my $timelimit_minutes = 0;
    if ((my $m = pop @tl)) { $timelimit_minutes += int(($m + 59) / 60); } # ceiling(seconds)
    if ((my $m = pop @tl)) { $timelimit_minutes += $m;                  } # minutes
    if ((my $m = pop @tl)) { $timelimit_minutes += ($m * 60);           } # hours
    if ((my $m = pop @tl)) { $timelimit_minutes += ($m * 24 * 60);      } # days
    $h{timelimit_minutes} = $timelimit_minutes;
    return(\%h);
}

# discover jobinfo for a single running job using SLURM squeue
sub runningJobInfo($) {
    my $j = shift;
    # nearly identical to finishedjobinfo, by design
    my $cmd = "squeue -o \"jobid=$j jobstate=%T username=%u account=%a nodes=%N procs=%C partition=%P jobname=%j maxmemory_in_GiB=not-set maxmemory_node=not-set timelimit=%l submit_time=not-set start_time=%S end_time=%e runtime=%M margin=not-set queuetime=not-set\" -M $o_cluster -h -j $j | sed '1d'";  # sed removes 'CLUSTER: cluster' that appears with the -M option
    return parseSqueueLine(qx($cmd));
}

# parse a single finishedjobinfo-formatted line and return a hash
sub parseFinishedjobinfoLine($) {
    my $line = shift;
    chomp $line;
    my (undef, undef, $keyvals) = split (/ /, $line, 3);  # removes 'date time ' at start
    return(0) if (! defined($keyvals));
    # note that null values "... nodes= ..." will result in no key in the hash
    my %h = $keyvals =~ /([^ ]+)=([^ ]+)/g;
    $h{jobname} = "'$h{jobname}'";
    $h{maxmemory_in_GiB} = 'not-set' if not exists $h{maxmemory_in_GiB};
    $h{maxmemory_node} = 'not-set' if not exists $h{maxmemory_node};
    return(\%h);
}

# discover jobinfo for a single job using finishedjobinfo script
sub jobInfo($) {
    my $j = shift;
    my $cmd = "$o_finishedjobinfo".($o_Quick ? " -q" : "")." -M $o_cluster -j $j";
    print STDERR "Running '$cmd' for more information, please be patient...\n";
    return parseFinishedjobinfoLine(qx($cmd));
}

# parse list of nodes returned by finishedjobinfo or provided on -n
sub parseNodes($) {
    my $nds = shift;
    # nodes=                                # nodes=m80
    # nodes=m[26,74-75,77-78,81-84,88-89]   # nodes=m[100-101,103-104]
    # nodes=m[57,135-137]                   # nodes=m[135-136]
    # nodes=m2,m3,m4,m5
    # nonsensical results if finishedjobinfo-style and comma-separated are mixed
    if ($nds !~ /\[/) { # 0 or 1 nodes or comma-separated
        return ( $nds =~ /,/ ? split(/,/, $nds) : $nds ); 
    }
    $nds =~ s/^(.+)\[(.*)\]$/$2/g;  # strip off 'prefix'[ and ]
    my $node_prefix = $1;
    my @node_nums;
    foreach my $p ( split /,/, $nds ) {
        my @r = split /-/, $p;
        push @node_nums, ($#r ? ($r[0] .. $r[1]) : $r[0]);
    }
    return map { $node_prefix . $_ } @node_nums;
}

# find jobstats files, if they exist
sub getJobstatsFiles($$@) {  # cluster jobid node-list
    my $clstr = shift; 
    my $jbid = shift; 
    my @node_list = @_;
    # print STDERR "cluster:$clstr jobid:$jbid nodelist:", join(",", @node_list), "\n";
    my @file_list;
    foreach my $node ( @node_list ) {
        my $fn = ( $o_hardprefix ? "$PREFIX/$jbid" : "$PREFIX/$node/$jbid" );
        -f $fn and -s $fn and push @file_list, $fn;
    }
    return @file_list;
}

# look in jobstats file for number of cores used
sub getJobstatsFileCoreCount($) {
    my $file = shift;
    open(F, "<$file") or die "***\n*** Could not open jobstats file $file: $!\n***";
    scalar(<F>); # header line
    my $l = <F>; # first data line
    # first 5 fields are LOCALTIME, TIME, GB_LIMIT, GB_USED, GB_SWAP_USED
    my @f = split /[ \t]+/, $l;
    return scalar(@f) - 5;
}

# reorder node list putting nodes with jobstats files first
sub reorderNodeList($$) {
    my $nl = shift; my $fl = shift;
    my @old_node_list = @{$nl};
    my @file_list = @{$fl};
    my @new_node_list;
    my %seen;
    foreach my $file ( @file_list ) {
        my @p = split("/", $file);
        push @new_node_list, $p[-2];  # node name
        ++$seen{$p[-2]};
    }
    push @new_node_list, grep { $seen{$_} ? () : $_ } @old_node_list;
    return(@new_node_list);
}

sub fieldNamesInOrder($) {
    my $f = shift;
    return grep {exists $f->{$_}} @field_order;  # removes empty field names when key is undefined
}
sub fieldContentsInOrder($) {
    my $f = shift;
    return @{$f}{fieldNamesInOrder($f)};
}

sub runPlotJobstats($) {
    my $fields = shift;
    # fields in %$fields
    # 0 = jobid, 1 = cluster, 2 = jobstate, 3 = user, 4 = project, 5 = jobname,
    # 6 = endtime, 7 = runtime, 8 = current flags, 9 = booked (amount booked?),
    # 10 = maxmem 11 = core_list number of cores per node, 12 = node_list node name(s),
    # 13 = file_list jobstats file(s)
    #
    # if this is for a currently-running job ($o_running and $o_source eq "squeue"),
    # there is an additional field:
    # 14 = timelimit_minutes
    # and maybe a first pair of arguments:
    #     "--timelimit-minutes  INT"
    -f "$o_plot_jobstats" or die "\n***\n*** Script for plotting jobstats '$o_plot_jobstats' not available\n***";
    my @args;
    if ($o_plot_stdin) {
        # plot_jobstats received job-specific info from stdin
        unshift @args, "--stdin";
    } else {
        # plot_jobstats received job-specific info from the argument list
        unshift @args, fieldContentsInOrder($fields);
    }
    unshift @args, $source_R_flags{$o_source};  # --fji or --squeue or --db, must be just before @_
    unshift @args, "--cpu-free", $o_cpufree;
    unshift @args, "--memory" if $o_memory;
    unshift @args, "--verbose" if $o_verbose;
    unshift @args, "--no-plot" if ! $o_plot;
    unshift @args, "--big-plot" if $o_bigplot;
    # all the above come before the passed-in args, if they apply
    print STDERR "$o_plot_jobstats ".join(' ', @args)."\n" if $o_debug;
    print STDERR "this.args = c(\"".join("\",\"", @args)."\")\n" if $o_debug;
    my $flags = qx($o_plot_jobstats @args);
    chomp $flags;
    return($flags);

}

sub print_JobNotRun($) {
    my $ji = shift;
    my %fields = (jobid         => $ji->{jobid},
                  cluster       => $o_cluster,
                  jobstate      => $ji->{jobstate},
                  user          => $ji->{username},
                  project       => $ji->{account},
                  jobname       => $ji->{jobname},
                  endtime       =>  ".",
                  runtime       =>  $ji->{runtime},
                  flag_list     => "not_run",
                  booked        => $ji->{procs},
                  maxmem        => ".",
                  core_list     => ".",
                  node_list     => ".",
                  jobstats_list => ".");
    $fields{timelimit_minutes} = "." if $o_running;
    print_JobRun(\%fields);
}

sub print_JobRun($) {
    my $f = shift;
    return if $o_quiet;
    if ($o_header) {  # don't print the header until ready to create output
        print STDOUT join("\t", fieldNamesInOrder($f)), "\n";
        $o_header = 0;
    }
    print STDOUT join("\t", fieldContentsInOrder($f)), "\n";
}

@nodes = parseNodes($o_nodes) if $o_nodes;

if ($o_project) {
    my $cmd = "$o_finishedjobinfo -q -M $o_cluster $o_project";
    print STDERR "Running '$cmd' through a pipe to get more information, please be patient...\n";
    open(FJI_PIPE, "$cmd |") or die "could not run $o_finishedjobinfo for project '$o_project': $!";
} elsif ($has_multijobs) {
    my $j = join(",", @jobs);
    my $cmd = "$o_finishedjobinfo -q -M $o_cluster -j $j";
    print STDERR "Running '$cmd' through a pipe to get more information, please be patient...\n";
    open(FJI_PIPE, "$cmd |") or die "could not run $o_finishedjobinfo for multiple jobs '$j': $!";
}

while ( 1 ) {

    my $jobid;
    my $input_line;

    if ($o_stdin) {
        $input_line = <>;
        last if ! $input_line;
    } elsif ($o_project or $has_multijobs) {
        $input_line = <FJI_PIPE>;
        last if ! $input_line;
    } else {
        last if ! @jobs;
        $jobid = shift @jobs;
    }

    ++$jobcount;

    # $o_cluster already declared above
    my $jobstate = '.';
    my $user = '.';
    my $project = '.';
    my $jobname = '.';
    my $endtime = '.';
    my $runtime = '.';
    my $booked = '.';
    my $total_cores = 0;
    my $maxmem = '.'; # will be set if not ($o_Quiet or $o_running)
    my @flag_list;
    my @core_list;
    my @node_list;
    my @file_list,
    my $timelimit_minutes = '.'; # only used if $o_running

    my @jobstats_list;

    if (@nodes) {  # -n was used, use this list
        @node_list = @nodes;
    } else {  # discover using finishedjobinfo or squeue or stdin
        my $ji;
        if ($o_stdin or $o_project or $has_multijobs) {
            $ji = parseFinishedjobinfoLine($input_line);
            die "***\n*** Jobid cannot be found, inconsistency in stdin input\n***" if ! $ji;
            $jobid = $ji->{jobid};
        } elsif ($o_running) {
            $ji = runningJobInfo($jobid);
            if (! $ji) {  # couldn't find jobinfo, give error and skip to next
                print STDERR "***\n*** Jobid $jobid cannot be found with squeue, it may not be running\n***\n";
                ++$jobcount_notrun;
                next;
            }
            if (! scalar(grep /^$ji->{account}$/, @user_groups) and ! $in_staff) {
                print STDERR "***\n*** Please, do not check other projects (jobid $jobid in $ji->{account})\n***\n";
                next;
            }
        } else {
            $ji = jobInfo($jobid);
            if (! $ji) {  # couldn't find jobinfo, give error and skip to next
                print STDERR "***\n*** Jobid $jobid cannot be found\n***\n";
                next;
            }
            if (! scalar(grep /^$ji->{account}$/, @user_groups) and ! $in_staff) {
                print STDERR "***\n*** Please, do not check other projects (jobid $jobid in $ji->{account})\n***\n";
                next;
            }
        }
        if (! defined($ji->{nodes})) {  # appears the job never started
            ++$jobcount_notrun;
            print_JobNotRun($ji);
            next;
        }
        @node_list = parseNodes($ji->{nodes});
        $jobstate = $ji->{jobstate};
        $user = $ji->{username};
        $project = $ji->{account}; 
        $jobname = $ji->{jobname};
        $endtime = $ji->{end_time};
        $runtime = $ji->{runtime};
        $booked = $ji->{procs};
        $maxmem = $ji->{maxmemory_in_GiB} if $ji->{maxmemory_in_GiB} ne 'not-set';
        $timelimit_minutes = $ji->{timelimit_minutes} if $o_running;
    }
    @file_list = getJobstatsFiles($o_cluster, $jobid, @node_list);
    @node_list = reorderNodeList(\@node_list, \@file_list) if not $o_hardprefix;
    foreach ( @file_list ) {
        my $cores = getJobstatsFileCoreCount($_);
        $total_cores += $cores;
        push @core_list, $cores;
    }
    if (! @file_list) { ++$no_jobstats_file; }

    # check flags that we can easily check here, overridden if there are R script results
    if (@file_list and @file_list < @node_list) {
        if ($o_verbose) {
            push @flag_list, scalar(@node_list) . " nodes booked but " . scalar(@file_list) . " used";
        } else {
            push @flag_list, "nodes_overbooked:" . scalar(@node_list) . ":" . scalar(@file_list);
        }
    }
    # jobid cluster endtime runtime flag-list booked cores node-list jobstats-file-list
    my %fields;
    $fields{jobid} = $jobid;
    $fields{cluster} = $o_cluster;
    $fields{jobstate} = $jobstate;
    $fields{user} = $user;
    $fields{project} = $project;
    $fields{jobname} = $jobname;
    $fields{endtime} = $endtime;
    $fields{runtime} = $runtime;
    $fields{flag_list} = (@flag_list ? join(",", @flag_list) : ".");
    $fields{booked} = $booked;
    $fields{maxmem} = $maxmem;
    $fields{core_list} = (@core_list ? join(",", @core_list) : ".");
    $fields{node_list} = join(",", @node_list);
    $fields{file_list} = (@file_list ? join(",", @file_list) : ".");
    $fields{timelimit_minutes} = $timelimit_minutes if $o_running;

    # if jobstats files, run plot_jobstats R script to check more flags and produce a plot
    if (@file_list) {

        my $pjs_flags = runPlotJobstats(\%fields);

        $fields{flag_list} = $pjs_flags if $pjs_flags;  # if using %fields
    }

    print_JobRun(\%fields);

}

if (not $o_quiet) {
    print STDERR "*** $jobcount total jobs, ".
                 "$jobcount_notrun jobs not ".($o_running ? "running" : "run").", ".
                 "$no_jobstats_file jobs had no jobstats files (includes jobs not ".($o_running ? "running" : "run").")\n";
}
if ($no_jobstats_file and $o_quiet) {
    print STDERR "*** No jobstats files found for $no_jobstats_file out of $jobcount jobs, limited resource usage diagnosis";
    print STDERR " and no plot produced" if $o_plot;
    print STDERR "\n";
}

